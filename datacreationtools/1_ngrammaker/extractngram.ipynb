{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9210\", timeout=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "mapping={}\n",
    "with open('ngram_analyzer.json') as f:\n",
    "    mapping = json.load(f)\n",
    "if es.indices.exists(index=\"ngram_analyzer\"):\n",
    "    es.indices.delete(index=\"ngram_analyzer\")\n",
    "es.indices.create(index=\"ngram_analyzer\", body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'token': '真理', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}, {'token': 'が', 'start_offset': 2, 'end_offset': 3, 'type': 'word', 'position': 1}, {'token': 'あなた', 'start_offset': 3, 'end_offset': 6, 'type': 'word', 'position': 2}, {'token': 'がた', 'start_offset': 6, 'end_offset': 8, 'type': 'word', 'position': 3}, {'token': 'を', 'start_offset': 8, 'end_offset': 9, 'type': 'word', 'position': 4}, {'token': '自由', 'start_offset': 9, 'end_offset': 11, 'type': 'word', 'position': 5}, {'token': 'に', 'start_offset': 11, 'end_offset': 12, 'type': 'word', 'position': 6}, {'token': 'する', 'start_offset': 12, 'end_offset': 14, 'type': 'word', 'position': 7}]}\n"
     ]
    }
   ],
   "source": [
    "phrase = \"真理があなたがたを自由にする\"\n",
    "# kuromojiを指定して解析テスト\n",
    "body_ = {\"analyzer\": \"kuromoji_analyzer\", \"text\": phrase}\n",
    "print(es.indices.analyze(index=\"ngram_analyzer\",body=body_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記の例では、/data/ngramviewer/txtdata/ディレクトリ以下に、資料単位でtarファイルにまとめられた各ページのテキストデータが格納されている形式を想定しています。\n",
    "\n",
    "スクリプトを実行すると、/data/ngramviewer/ngramdata/ディレクトリ以下に、資料単位のgzip圧縮されたjsonとして、当該資料に出現する1gramから5gramまでのキーワードの頻度情報が、キーバリュー形式で格納されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "INPUTDIR=\"/data/ngramviewer/txtdata/\"\n",
    "OUTPUTDIR=\"/data/ngramviewer/ngramdata/\"\n",
    "os.makedirs(OUTPUTDIR,exist_ok=True)\n",
    "\n",
    "def is_num(s):\n",
    "    try:\n",
    "        float(s)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "for inputtarpath in glob.glob(os.path.join(INPUTDIR,\"*.tar\")):\n",
    "    pid=os.path.basename(inputtarpath).split(\".\")[0]\n",
    "    resdata={}\n",
    "    with tarfile.open(inputtarpath, 'r') as tr:\n",
    "        tarlistinfo=[]\n",
    "        for tarinfo in tr:\n",
    "            if \"txt\" in tarinfo.name:\n",
    "                tarlistinfo.append(tarinfo.name)\n",
    "        sorted(tarlistinfo)\n",
    "        resstr=\"\"\n",
    "        for tarinfo in tarlistinfo:\n",
    "            #print(tarinfo)\n",
    "            binary = tr.extractfile(tarinfo).read()\n",
    "            tmpstr = binary.decode(\"utf-8\").replace(\"\\t\",\"\").replace(\"\\n\",\"\")\n",
    "            resstr+=tmpstr\n",
    "        body_ = {\"analyzer\": \"kuromoji_analyzer\", \"text\": resstr}\n",
    "        resdata=es.indices.analyze(index=\"ngram_analyzer\",body=body_,request_timeout=1000)\n",
    "        resdataarray=[tmp[\"token\"] for tmp in resdata[\"tokens\"]]\n",
    "    arraylen=len(resdataarray)\n",
    "    countdict={}\n",
    "    for pos in range(arraylen):\n",
    "        tmpstr=\"\"\n",
    "        for i in range(5):#posの位置を先頭にして5gramまで集計の対象とする\n",
    "            if pos+i>=arraylen or is_num(resdataarray[pos+i]):\n",
    "                break\n",
    "            tmpstr+=(resdataarray[pos+i]+\" \")\n",
    "            if not tmpstr in countdict:\n",
    "                countdict[tmpstr]=1\n",
    "            else:\n",
    "                countdict[tmpstr]+=1\n",
    "    resjson = json.dumps(countdict,ensure_ascii=False, indent=2)\n",
    "    with gzip.open(os.path.join(OUTPUTDIR,pid+\".json.gz\"), mode='wt') as fp:\n",
    "        fp.write(resjson)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
